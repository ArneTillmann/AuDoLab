{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from AuDoLab import AuDoLab\r\n",
    "\r\n",
    "audo = AuDoLab.AuDoLab()\r\n",
    "print(audo.is_notebook())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    # Load target data from reuters dataset\r\n",
    "    from nltk.corpus import reuters\r\n",
    "    import numpy as np\r\n",
    "    import pandas as pd\r\n",
    "\r\n",
    "    data = []\r\n",
    "\r\n",
    "    for fileid in reuters.fileids():\r\n",
    "        tag, filename = fileid.split(\"/\")\r\n",
    "        data.append(\r\n",
    "            (filename,\r\n",
    "                \", \".join(\r\n",
    "                    reuters.categories(fileid)),\r\n",
    "                reuters.raw(fileid)))\r\n",
    "\r\n",
    "    # store loaded data in dataframe\r\n",
    "    data = pd.DataFrame(data, columns=[\"filename\", \"categories\", \"text\"])\r\n",
    "\r\n",
    "    #####------\r\n",
    "    # start using audolab\r\n",
    "\r\n",
    "    # clean theloaded data\r\n",
    "    preprocessed_target = audo.text_cleaning(data=data, column=\"text\")\r\n",
    "\r\n",
    "    # scrape ieee\r\n",
    "    scraped_documents = audo.get_ieee(pages=1)\r\n",
    "\r\n",
    "    # clean the scraped papers\r\n",
    "    preprocessed_paper = audo.text_cleaning(data=scraped_documents, column=\"abstract\")\r\n",
    "\r\n",
    "    # calculate tfidf values on joint corpus\r\n",
    "    target_tfidf, training_tfidf = audo.tf_idf(\r\n",
    "        data=preprocessed_target,\r\n",
    "        papers=preprocessed_paper,\r\n",
    "        data_column=\"lemma\",\r\n",
    "        papers_column=\"lemma\",\r\n",
    "        features=100000,\r\n",
    "    )\r\n",
    "\r\n",
    "    # calculate one_class_svm on data\r\n",
    "    o_svm_result = audo.one_class_svm(\r\n",
    "        training=training_tfidf,\r\n",
    "        predicting=target_tfidf,\r\n",
    "        nus=np.round(np.arange(0.001, 0.5, 0.01), 7),\r\n",
    "        quality_train=0.9,\r\n",
    "        min_pred=0.001,\r\n",
    "        max_pred=0.05,\r\n",
    "    )\r\n",
    "\r\n",
    "    # select a classifier\r\n",
    "    result = audo.choose_classifier(preprocessed_target, o_svm_result, 0)\r\n",
    "\r\n",
    "    # perform topic modeling and plot the created topics\r\n",
    "    lda_target = audo.lda_modeling(data=result, num_topics=5)\r\n",
    "    #audo.lda_visualize_topics(type=\"clouds\", n_clouds=4)\r\n",
    "    audo.lda_visualize_topics(type=\"pyldavis\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10788/10788 [00:04<00:00, 2255.92it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The algorithm is iterating through 1 pages\n",
      "Total number of abstracts that will be scraped: 100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n",
      "100%|██████████| 93/93 [00:00<00:00, 2142.43it/s]\n",
      "100%|██████████| 93/93 [00:00<00:00, 1972.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nu: 0.151 data predicted: 27 training_data predicted: 89\n",
      "nu: 0.291 data predicted: 28 training_data predicted: 89\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('AuDoLab': conda)"
  },
  "interpreter": {
   "hash": "897036928a98709e55f1a20e866055656ff707a6697846f8103ec94cffdc2255"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}