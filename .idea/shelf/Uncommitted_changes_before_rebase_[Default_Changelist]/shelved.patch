Index: paper.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>---\ntitle: 'AuDoLab: Automatic document labelling and classfication for extremely unbalanced data'\ntags:\n  - Python\n  - One-class SVM\n  - Unsupervised Document Classification\n  - One-class Document Classification\n  - LDA Topic Modelling\n  - Out-of-domain Training Data\nauthors:\n  - name: Arne Tillmann^[Custom footnotes for e.g. denoting who the corresponding author is can be included like this.]\n    orcid: 0000-0003-0872-7098\n    affiliation: 1\n  - name: Anton Thielmann\n    affiliation: 1\n  - name: Christoph Weisser\n    affiliation: 1,2\n  - name: Benjamin Säfken\n    affiliation: 1,2\n  - name: Thomas Kneib\n    affiliation: 1,2\n  - name: Alexander Silbersdorff\n    affiliation: 1\naffiliations:\n - name: Georg-August-Universität Göttingen, Göttingen, Germany\n   index: 1\n - name: Campus-Institut Data Science (CIDAS), Göttingen, Germany\n   index: 2\ndate: 24 April 2021\nbibliography: paper.bib\n\n# Optional fields if submitting to a AAS journal too, see this blog post:\n# https://blog.joss.theoj.org/2018/12/a-new-collaboration-with-aas-publishing\naas-doi: 10.3847/xxxxx <- update this with the DOI from AAS once you know it.\naas-journal: Astrophysical Journal <- The name of the AAS journal.\n---\n\n# Summary\n\nIn Natural Language Processing (NLP) Unsupervised Document Classification is mainly done on large and balanced datasets.\nAuDoLab tackles this problem and provides a novel approach to one-class document classification for heavily imbalanced datasets.\n\nFurthermore, AuDoLab enables the user to  create  user specific out-of-domain training data and classify a heavily underrepresented target class\nin a target dataset, using Web Scraping, Latent Dirichlet Allocation Topic Modelling and one-class Support Vector Machines.\n\n\n\n\nAuDoLab can be used for various applications for scientific research and also has various business applications.\nThe following section provides an detailed overview of AuDoLab. Subsequently, it will be discussed how the\ntheoretical models behind AuDoLab advance existing methods and software solutions. AuDoLab can be installed conveniently via pip.\n\n\n\n installation and the package can be found in the packages repository or on the documentation website of TTLocVis\n\n\n# Statement of need\n\nUnsupervised document classification is mainly performed to gain insight into the underlying topics of large text corpora.\nIn this process, highly underrepresented topics are often overlooked and consequently assigned to the wrong topics. Thus, labeling underrepresented topics in large text corpora is often done manually and can therefore be very time-consuming. AuDoLab enables the user to tackle this problem and perform unsupervised one-class document classification for heavily underrepresented document classes. This leverages the results of one-class document classification using one-class support vector machines (SVM) [@Scholkopf][@Manevitz] and extends them to the use case of severely imbalanced datasets. This adaptation and extension is achieved by implementing a multi-level classification rule as shown below.\n\n![Classification Procedure.\\label{fig:test2}](figures/tree.PNG){ width=100% }\n\n \nFirstly, the package enables the user to web scrape training documents (scientific papers) from IEEEXplore. The user can search for multiple search terms and specify an individual search query. Thus, one can create its own, individually labelled (e.g. via author-keywords) training data set. Through the integration of pre-labelled out-of-domain training data, the problem of the heavily underrepresented target class can be circumvented, as large enough training corpora can be automatically generated.\nSubsequently, the text data is preprocessed for the classification part. The text preprocessing includes common NLP text preprocessing techniques as stopword removal and lemmatization.  As  document  representations  the  term  frequency-inverse  document  frequency  (tf-idf) representations are chosen. The tf-idf scores are computed on a joint corpus from the web-scraped out-of-domain training data and the target text data. \n\nThe main part of the classification rule lies in the training of the one-class SVM [@Scholkopf]. Aa a training corpus, only the out-of-domain training data is used. With setting the right hyperparameters, the user can create a strict or relaxed classification rule, based on the users belief of the prevalence of the target class inside the target data set and the quality of the scraped out-of-domain training data. The last part of the classification rule enables the user to control the classifiers results with the help of LDA topic models [@Blei] (and e.g. wordclouds). Additionally, the user can generate interactive plots depicicting the identified topics during the LDA topic modelling [@ldavis].\n\nThe second step can thus be reiterade a desirable number of times, depending on the users perceived quality of the classification results.\n\n# Comparison with existing tools\n\n\n\n\n\n# Citations\n\nCitations to entries in paper.bib should be in\n[rMarkdown](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html)\nformat.\n\nIf you want to cite a software repository URL (e.g. something on GitHub without a preferred\ncitation) then you can do it with the example BibTeX entry below for @fidgit.\n\nFor a quick reference, the following citation commands can be used:\n- `@author:2001`  ->  \"Author et al. (2001)\"\n- `[@author:2001]` -> \"(Author et al., 2001)\"\n- `[@author1:2001; @author2:2001]` -> \"(Author1 et al., 2001; Author2 et al., 2002)\"\n\n# Figures\n\nFigures can be included like this:\n![Caption for example figure.\\label{fig:example}](figure.png)\nand referenced from text using \\autoref{fig:example}.\n\nFigure sizes can be customized by adding an optional second parameter:\n![Caption for example figure.](figure.png){ width=20% }\n\n# Acknowledgements\n\nWe acknowledge contributions from Brigitta Sipocz, Syrtis Major, and Semyeong\nOh, and support from Kathryn Johnston during the genesis of this project.\n\n# References\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- paper.md	(revision c814480f1a66f59bbbdffbe7a1f9a73f128f1f3d)
+++ paper.md	(date 1619270379163)
@@ -10,23 +10,25 @@
 authors:
   - name: Arne Tillmann^[Custom footnotes for e.g. denoting who the corresponding author is can be included like this.]
     orcid: 0000-0003-0872-7098
-    affiliation: 1
+    affiliation: "1, 2" # (Multiple affiliations must be quoted)
   - name: Anton Thielmann
-    affiliation: 1
+    affiliation: 2
   - name: Christoph Weisser
-    affiliation: 1,2
+    affiliation: 3
   - name: Benjamin Säfken
-    affiliation: 1,2
+    affiliation: 3
   - name: Thomas Kneib
-    affiliation: 1,2
+    affiliation: 3
   - name: Alexander Silbersdorff
-    affiliation: 1
+    affiliation: 3
 affiliations:
- - name: Georg-August-Universität Göttingen, Göttingen, Germany
+ - name: Lyman Spitzer, Jr. Fellow, Princeton University
    index: 1
- - name: Campus-Institut Data Science (CIDAS), Göttingen, Germany
+ - name: Institution Name
    index: 2
-date: 24 April 2021
+ - name: Independent Researcher
+   index: 3
+date: 13 August 2017
 bibliography: paper.bib
 
 # Optional fields if submitting to a AAS journal too, see this blog post:
@@ -58,17 +60,20 @@
 # Statement of need
 
 Unsupervised document classification is mainly performed to gain insight into the underlying topics of large text corpora.
-In this process, highly underrepresented topics are often overlooked and consequently assigned to the wrong topics. Thus, labeling underrepresented topics in large text corpora is often done manually and can therefore be very time-consuming. AuDoLab enables the user to tackle this problem and perform unsupervised one-class document classification for heavily underrepresented document classes. This leverages the results of one-class document classification using one-class support vector machines (SVM) [@Scholkopf][@Manevitz] and extends them to the use case of severely imbalanced datasets. This adaptation and extension is achieved by implementing a multi-level classification rule as shown below.
+In this process, highly underrepresented topics are often overlooked and consequently assigned to the wrong topics.
+Thus, labeling underrepresented topics in large text corpora is often done manually and can therefore be very time-consuming.
 
 ![Classification Procedure.\label{fig:test2}](figures/tree.PNG){ width=100% }
 
- 
-Firstly, the package enables the user to web scrape training documents (scientific papers) from IEEEXplore. The user can search for multiple search terms and specify an individual search query. Thus, one can create its own, individually labelled (e.g. via author-keywords) training data set. Through the integration of pre-labelled out-of-domain training data, the problem of the heavily underrepresented target class can be circumvented, as large enough training corpora can be automatically generated.
-Subsequently, the text data is preprocessed for the classification part. The text preprocessing includes common NLP text preprocessing techniques as stopword removal and lemmatization.  As  document  representations  the  term  frequency-inverse  document  frequency  (tf-idf) representations are chosen. The tf-idf scores are computed on a joint corpus from the web-scraped out-of-domain training data and the target text data. 
+AuDoLab enables the user to tackle this problem and perform unsupervised one-class document classification for heavily underrepresented document classes.
+Firstly, the package enables the user to web scrape training documents (scientific papers) from IEEEXplore.
+The user can search for multiple search terms and specify an individual search query. Subsequently, the text data is preprocessed for the classification part.
+The text preprocessing includes common NLP text preprocessing techniques as stopword removal and lemmatization.
+As  document  representations  the  term  frequency-inverse  document  frequency  (tf-idf) representations are chosen.
+The tf-idf scores are computed on a joint corpus from the web-scraped out-of-domain training data and the target text data.
 
-The main part of the classification rule lies in the training of the one-class SVM [@Scholkopf]. Aa a training corpus, only the out-of-domain training data is used. With setting the right hyperparameters, the user can create a strict or relaxed classification rule, based on the users belief of the prevalence of the target class inside the target data set and the quality of the scraped out-of-domain training data. The last part of the classification rule enables the user to control the classifiers results with the help of LDA topic models [@Blei] (and e.g. wordclouds). Additionally, the user can generate interactive plots depicicting the identified topics during the LDA topic modelling [@ldavis].
-
-The second step can thus be reiterade a desirable number of times, depending on the users perceived quality of the classification results.
+The actual document classification is performed using one-class Support vector machines,
+trained on the tf-idf representations from the out-of-domain training data, that are computed on the joint corpus.
 
 # Comparison with existing tools
 
